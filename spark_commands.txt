


val df = spark.readStream
        .format("kafka")
        .option("kafka.bootstrap.servers", "localhost:9092")
        .option("subscribe", "test-mysql-jdbc-coffee")
        .option("startingOffsets", "latest") 
        .load()
		
val df = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "localhost:9092").option("subscribe", "test-mysql-jdbc-coffee").option("startingOffsets", "latest").load()		
		
val schema = new StructType()
      .add("id",IntegerType)
      .add("product",StringType)
      .add("price",DoubleType)
      .add("ptime",LongType)


val coffeeDF = coffes.select("payload.id","payload.price","payload.product","payload.ptime").collect()

val coffeeNewDF = df.select(coffeeDF, schema).as("data"))
   .select("data.*")


 
 
val coffeeWin = coffeeNewDF.groupBy(window('Date, "1 hour"),product).
			.agg(sum("price").as("hourly_sum"),count(*).as(hourly_cnt))

			
val coffeeNewSUM = coffeeWin.selectExpr("window.start" as ptime,"product","hourly_sum","hourly_cnt")

coffeeNewSUM.write.format("csv").option("sep",",").option("header","true").save("/home/ozhan/coffeesumdata/")


val coffeeReadedDF = spark.read.format("csv").option("header", "true")
                               .option("inferSchema","true")
							   .load("/home/ozhan/coffeesumdata/")
   
coffeeReadedDF.createOrReplaceTempView("coffeeReadedDFTemp")

val resDF = spark.sql("select product,from_unixtime(ptime,'yyyyMMdd') as mtime,sum(hourly_sum),sum(hourly_cnt) from coffeeReadedDFTemp group by product,from_unixtime(ptime,'yyyyMMdd') order by mtime")
resDF.write.format("csv").option("sep",",").option("header","true").save("/home/ozhan/coffeeresdata/")




